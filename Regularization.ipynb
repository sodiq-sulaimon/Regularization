{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87e4b03",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455ed9f",
   "metadata": {},
   "source": [
    "#### 1. Non-Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb082927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n",
    "                                                # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094204b",
   "metadata": {},
   "source": [
    "#### 2. L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc16b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    L2_regularization_cost = (1/m) * (lambd/2) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "        \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84251ddb",
   "metadata": {},
   "source": [
    "#### 3. Back-Propagation with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b92f46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "\n",
    "    dW3 = (1./m) * np.dot(dZ3, A2.T) + ((lambd /m) * W3)\n",
    "    \n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    \n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + ((lambd /m) * W2)\n",
    "    \n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + ((lambd /m) * W1)\n",
    "    \n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c56950",
   "metadata": {},
   "source": [
    "#### 4. Forward Propagation with Drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7414ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])   # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob).astype(int)               # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = np.multiply(A1, D1)                        # Step 3: shut down some neurons of A1\n",
    "    A1 /=  keep_prob                                # Step 4: scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n",
    "    D2 = (D2 < keep_prob).astype(int)\n",
    "    A2 = np.multiply(A2, D2)\n",
    "    A2 /=  keep_prob\n",
    "    \n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b508ce9",
   "metadata": {},
   "source": [
    "**Things to remember**:\n",
    "- Regularization will help you reduce overfitting.\n",
    "- Regularization will drive your weights to lower values.\n",
    "- L2 regularization and Dropout are two very effective regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdaf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
